{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_3kWCGSBg6d"
      },
      "source": [
        "## Text summarization using Hugging face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgOltPKaCvNG"
      },
      "source": [
        "Ensure you are using GPU - for faster training capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmMrIqSIAU2h"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2ZgQRyoDbSr"
      },
      "source": [
        "Install dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHRJqBJ2DNX9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NyCMOTMEMjN"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate\n",
        "!pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RP7b81bE0sF"
      },
      "source": [
        "Import required tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB71YhVlEdAF"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from transformers import pipeline, set_seed, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from datasets import load_dataset, load_from_disk, load_metric\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b2t8_jgGHOq"
      },
      "source": [
        "check if GPU is running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K245aoSF-37"
      },
      "outputs": [],
      "source": [
        "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXZs1OVtHSAg"
      },
      "source": [
        "# **Pre-Processing Data**\n",
        "\n",
        "\n",
        "1.   Tokenization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlzLExYUGExw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTZB5ZbzIMFS"
      },
      "outputs": [],
      "source": [
        "model = \"google/pegasus-cnn_dailymail\"      # model we shall use for summarization\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxZ36TwbNY5S"
      },
      "outputs": [],
      "source": [
        "model_p = AutoModelForSeq2SeqLM.from_pretrained(model).to(dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7acr9echx8h"
      },
      "source": [
        "load the dataset to be summarized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGrdfXCekDdF"
      },
      "outputs": [],
      "source": [
        "!pip install fsspec==2023.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_21z7hvkKI0"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset('abisee/cnn_dailymail', '3.0.0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cofoFuKwkvNk"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUO1Zh1ClyXX"
      },
      "source": [
        "**visualizing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3HT0PwvlhXX"
      },
      "outputs": [],
      "source": [
        "ds['train']['highlights'][10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubT5jQDZlpkg"
      },
      "outputs": [],
      "source": [
        "ds['train'][10][\"highlights\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T33b5n-Uohxn"
      },
      "source": [
        "**Convert to vector representation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM2vAHnAmZvP"
      },
      "outputs": [],
      "source": [
        "def convert_to_features(example_batch):\n",
        "    input_encodings = tokenizer(example_batch['article'], max_length=1024, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encodings = tokenizer(example_batch['highlights'], max_length=128, truncation=True)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_encodings['input_ids'],\n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'labels': target_encodings['input_ids']\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWQuUBu9rKJa"
      },
      "source": [
        "Lets map the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pPmBEzfrIjs"
      },
      "outputs": [],
      "source": [
        "ds_cn = ds.map(convert_to_features, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJRXSncmraBT"
      },
      "outputs": [],
      "source": [
        "ds_cn['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU-O0f-2v-vt"
      },
      "outputs": [],
      "source": [
        "#ds_cn['train']['input_ids'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeBUMm2swMBE"
      },
      "outputs": [],
      "source": [
        "#ds_cn['train']['attention_mask'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doK0sfeVw3M3"
      },
      "outputs": [],
      "source": [
        "ds_cn['train']['labels'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtno5wAaxaGd"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ez2XMDh6eTG"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWfZQAUZxZxv"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq # load data in batches - u may have huge amount of data\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpZH-WbCzrZw"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import transformers\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='pg_tst',\n",
        "    num_train_epochs=1, # keep everything same only this field may change\n",
        "    warmup_steps=500,\n",
        "    per_device_train_batch_size=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    eval_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    save_steps=1e6,\n",
        "    gradient_accumulation_steps=16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqEjka1m7WAL"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model_p, args=training_args,\n",
        "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=ds_cn['test'], eval_dataset=ds_cn['validation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T1VKear7zAA"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pveL4lDl8EkG"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "\n",
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]\n",
        "\n",
        "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
        "                               batch_size=16, device=dev,\n",
        "                               column_text=\"article\",\n",
        "                               column_summary=\"highlights\"):\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)):\n",
        "\n",
        "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
        "                        padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
        "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
        "\n",
        "        # Decode generated texts\n",
        "\n",
        "        # replace the token and add the decoded  text with refrence to the matrix\n",
        "\n",
        "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
        "                                clean_up_tokenization_spaces=True)\n",
        "               for s in summaries]\n",
        "\n",
        "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "\n",
        "\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "        # Compute and return rogue scores\n",
        "        score = metric.compute()\n",
        "        return score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "rouge_metric = load_metric('rouge')\n"
      ],
      "metadata": {
        "id": "r_1Q_OTFBz3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_metric_on_test_ds(\n",
        "    ds_cn['validation'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'article', column_summary= 'highlights')\n",
        "\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
        "\n",
        "pd.DataFrame(rouge_dict, index = [f'pg_tst'] )"
      ],
      "metadata": {
        "id": "b9Nu_hXBFOm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ykVj7IONkiO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "model_p.save_pretrained(\"pg_tst\")"
      ],
      "metadata": {
        "id": "AM1cMHwHmAQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save tokenizer\n",
        "tokenizer.save_pretrained(\"tokenizer\")"
      ],
      "metadata": {
        "id": "eHyNNpY3my3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")"
      ],
      "metadata": {
        "id": "yHGq9X7JnKce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction\n",
        "\n",
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
        "\n",
        "sample_txt = ds['test'][0]['article']\n",
        "\n",
        "reference = ds['test'][0]['highlights']\n",
        "\n",
        "pipe = pipeline(\"summarization\", model=\"pg_tst\",tokenizer=tokenizer)\n",
        "\n",
        "##\n",
        "print(\"Dialogue:\")\n",
        "print(sample_txt)\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_txt, **gen_kwargs)[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "qYTWouTAnQt3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}